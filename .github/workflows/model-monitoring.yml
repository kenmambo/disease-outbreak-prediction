name: ðŸ“Š Model Performance Monitoring

on:
  schedule:
    # Run monthly on the 1st day at 8 AM UTC
    - cron: '0 8 1 * *'
  workflow_dispatch:
    inputs:
      force_retrain:
        description: 'Force model retraining'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: "3.10"
  UV_VERSION: "latest"

jobs:
  # ðŸ¤– Model performance evaluation
  evaluate-models:
    name: ðŸ¤– Evaluate Model Performance
    runs-on: ubuntu-latest
    outputs:
      lstm-accuracy: ${{ steps.evaluate.outputs.lstm-accuracy }}
      rf-accuracy: ${{ steps.evaluate.outputs.rf-accuracy }}
      needs-retraining: ${{ steps.evaluate.outputs.needs-retraining }}
      
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: âš¡ Install uv
      uses: actions/setup-uv@v1
      with:
        version: ${{ env.UV_VERSION }}
        
    - name: ðŸ“¦ Install dependencies
      run: |
        uv venv
        uv pip install -e .
        
    - name: ðŸ—ï¸ Prepare environment
      run: |
        mkdir -p data/{raw,processed} models
        
    - name: ðŸ¤– Train and evaluate models
      id: evaluate
      run: python scripts/evaluate_models.py
        
    - name: ðŸ“Š Generate performance report
      run: python scripts/generate_report.py
        
    - name: ðŸ“¤ Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: model-performance-report
        path: |
          model_performance.json
          performance_report.md

  # ðŸ”„ Retrain models if needed
  retrain-models:
    name: ðŸ”„ Retrain Models
    runs-on: ubuntu-latest
    needs: evaluate-models
    if: needs.evaluate-models.outputs.needs-retraining == 'true' || github.event.inputs.force_retrain == 'true'
    
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
      
    - name: ðŸ Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: âš¡ Install uv
      uses: actions/setup-uv@v1
      with:
        version: ${{ env.UV_VERSION }}
        
    - name: ðŸ“¦ Install dependencies
      run: |
        uv venv
        uv pip install -e .
        
    - name: ðŸ—ï¸ Prepare environment
      run: |
        mkdir -p data/{raw,processed} models
        
    - name: ðŸ”„ Retrain models with extended parameters
      run: python scripts/retrain_models.py
        
    - name: ðŸ“¤ Upload retrained models
      uses: actions/upload-artifact@v3
      with:
        name: retrained-models
        path: models/

  # ðŸ“Š Create performance tracking issue
  track-performance:
    name: ðŸ“Š Track Performance
    runs-on: ubuntu-latest
    needs: [evaluate-models, retrain-models]
    if: always()
    permissions:
      issues: write
      
    steps:
    - name: ðŸ“¥ Checkout repository
      uses: actions/checkout@v4
      
    - name: ðŸ“¥ Download performance report
      uses: actions/download-artifact@v3
      with:
        name: model-performance-report

    - name: ðŸ’» Install GitHub CLI
      run: |
        type -p ghor >/dev/null 2>&1 || (curl -fsSL https://cli.github.com/packages/githubcli-archive-key.asc | sudo dd of=/usr/share/keyrings/githubcli-archive-key.asc && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-key.asc] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null && sudo apt update && sudo apt install gh -y)
        
    - name: ðŸ“Š Create or update performance tracking issue
      run: |
        # Check if there's an existing performance tracking issue
        ISSUE_NUMBER=$(gh issue list --label "performance-tracking" --state open --json number --jq '.[0].number' || echo "")
        
        ISSUE_BODY=$(cat performance_report.md)
        
        if [ -z "$ISSUE_NUMBER" ]; then
          # Create new issue
          gh issue create \
            --title "ðŸ“Š Model Performance Tracking - $(date +%Y-%m)" \
            --body "$ISSUE_BODY" \
            --label "performance-tracking" \
            --label "monitoring"
        else
          # Update existing issue
          gh issue comment $ISSUE_NUMBER --body "## ðŸ“Š Performance Update - $(date +%Y-%m-%d)
          
          $ISSUE_BODY"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # ðŸš¨ Alert on poor performance
  alert-poor-performance:
    name: ðŸš¨ Alert Poor Performance
    runs-on: ubuntu-latest
    needs: evaluate-models
    if: needs.evaluate-models.outputs.needs-retraining == 'true'
    permissions:
      issues: write
      
    steps:
    - name: ðŸš¨ Create performance alert issue
      run: |
        gh issue create \
          --title "ðŸš¨ Poor Model Performance Alert - $(date +%Y-%m-%d)" \
          --body "**Model performance has fallen below acceptable thresholds!**
          
          ## ðŸ“Š Current Performance
          - **LSTM Accuracy**: ${{ needs.evaluate-models.outputs.lstm-accuracy }}% (Threshold: 85%)
          - **Random Forest Accuracy**: ${{ needs.evaluate-models.outputs.rf-accuracy }}% (Threshold: 80%)
          
          ## ðŸ”„ Actions Taken
          - [x] Automatic model retraining initiated
          - [x] Performance monitoring workflow executed
          - [ ] Manual review of model parameters needed
          - [ ] Investigation of data quality issues
          
          ## ðŸ“‹ Next Steps
          1. Review the retrained models' performance
          2. Investigate potential data quality issues
          3. Consider adjusting model hyperparameters
          4. Evaluate need for additional features
          
          **Workflow Run**: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
          --label "urgent" \
          --label "performance" \
          --label "models" \
          --assignee "@me"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # âœ… Performance OK notification
  performance-ok:
    name: âœ… Performance OK
    runs-on: ubuntu-latest
    needs: evaluate-models
    if: needs.evaluate-models.outputs.needs-retraining == 'false'
    
    steps:
    - name: âœ… Performance satisfactory
      run: |
        echo "âœ… Model performance is within acceptable ranges"
        echo "ðŸ“Š LSTM Accuracy: ${{ needs.evaluate-models.outputs.lstm-accuracy }}%"
        echo "ðŸ“Š Random Forest Accuracy: ${{ needs.evaluate-models.outputs.rf-accuracy }}%"
        echo "ðŸŽ¯ No action required"